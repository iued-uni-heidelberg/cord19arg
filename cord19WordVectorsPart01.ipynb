{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.3 64-bit"
    },
    "interpreter": {
      "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
    },
    "colab": {
      "name": "cord19WordVectorsPart01.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iued-uni-heidelberg/cord19arg/blob/main/cord19WordVectorsPart01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty2YWcLcvKl4"
      },
      "source": [
        "# Word Vectors\n",
        "\n",
        "Word vectors (also known as 'word embeddings') are one of the most popular kinds of AI models. They are extremely useful in many domains. In essence, a word vector is a set of numbers that attempt to capture the meaning of a word. In typical implementations, each word is represented by a set of 200-300 numbers. In linear algebra, a one-dimensional array of numbers is known as a 'vector', hence these sets of numbers representing words' meanings are known as 'word vectors'.\n",
        "\n",
        "Using neural networks, we can expose the computer to a large amount of text, and allow it to learn an appropriate set of numbers for each word it encounters. In this notebook, we will learn about the most famous of all word vector algorithms, `word2vec`, which was first described by Tomas Mikolov and his team in 2013:\n",
        "\n",
        "* Tomas Mikolov, Ilya Sutskever, and others, â€˜Distributed Representations of Words and Phrases and Their Compositionalityâ€™, in Advances in Neural Information Processing Systems 26, ed. by C. J. C. Burges and others (Curran Associates, Inc., 2013), pp. 3111â€“19 <http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf>\n",
        "* Tomas Mikolov, Kai Chen, and others, â€˜Efficient Estimation of Word Representations in Vector Spaceâ€™, ArXiv:1301.3781 Cs, 2013 <http://arxiv.org/abs/1301.3781>.\n",
        "\n",
        "In fact, `word2vec` is not a single algorithm, but rather a family of similar algorithms. In this session we will consider just the most famous `word2vec` algorithm, namely the `skip-gram model` trained using `negative sampling`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWLKKQ0lvKl5"
      },
      "source": [
        "## Applications of Word Vectors\n",
        "\n",
        "Word vectors allow the computer to 'understand' language far more effectively. Rather than seeing each word as simply an arbitrarily different object, a computer using word vectors can analyse each word as a point in 200- or 300-dimenstional space. Words that are similar in meaning will have similar word vectors. And as we will see, the spaces between the word vectors are also significant: the words are arranged in patterns that represent their relationships to one another.\n",
        "\n",
        "Accordingly, most AI systems that process language now include a word vector layer as part of their architecure. When the system encounters some text (e.g. when you speak to Siri or Alexa), your words are converted into word vectors, *and then* the computer examines what the text says and determines how it should respond.\n",
        "\n",
        "In the Humanities, word vectors have become a popular modelling tool, because they allow researchers to perform sophisticated analysis on large corpora of text. Some examples include:\n",
        "\n",
        "* [The Women Writers Vector Toolkit](https://wwp.northeastern.edu/lab/wwvt/index.html)\n",
        "* William L. Hamilton, Jure Leskovec, and Dan Jurafsky, â€˜Diachronic Word Embeddings Reveal Statistical Laws of Semantic Changeâ€™, ArXiv:1605.09096 [Cs], 2018 <http://arxiv.org/abs/1605.09096>.\n",
        "* Ryan Heuser, 'Semantic Networks' <https://ryanheuser.org/word-vectors-4/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ZWncLevKl5"
      },
      "source": [
        "## Training a `word2vec` model in Gensim\n",
        "\n",
        "It is very easy to train a `word2vec` model in Gensim, which includes Mikolov's original `word2vec` code in its codebase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF5AOwyWH2l4"
      },
      "source": [
        "## CORD corpus models\n",
        "modify this to merge more files into your corpus "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzsJM-_MH6un",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5639ec45-f995-4bab-d59c-d3c1af91e66d"
      },
      "source": [
        "!rm /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eH5fU-jJjev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45ccf25-0a9d-460e-85ee-bbaf4f6e3f89"
      },
      "source": [
        "!wget https://heibox.uni-heidelberg.de/f/e9643d5305d94c5cb787/?dl=1\n",
        "!mv index.html?dl=1 file1.zip\n",
        "!wget https://heibox.uni-heidelberg.de/f/76525e4278114c438f1c/?dl=1\n",
        "!mv index.html?dl=1 file2.zip\n",
        "!wget https://heibox.uni-heidelberg.de/f/062f72f9e79e4ba5ae4e/?dl=1\n",
        "!mv index.html?dl=1 file3.zip\n",
        "!wget https://heibox.uni-heidelberg.de/f/4e290ad989a046caaef4/?dl=1\n",
        "!mv index.html?dl=1 file4.zip\n",
        "!wget https://heibox.uni-heidelberg.de/f/5ec055c8956348d98a44/?dl=1\n",
        "!mv index.html?dl=1 file5.zip\n",
        "\n",
        "!unzip file1.zip\n",
        "!unzip file2.zip\n",
        "!unzip file3.zip\n",
        "!unzip file4.zip\n",
        "!unzip file5.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-23 11:00:01--  https://heibox.uni-heidelberg.de/f/e9643d5305d94c5cb787/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/f7e0f408-0e5a-4893-95d6-892ff91c11a4/covid1_lem.txt.zip [following]\n",
            "--2021-09-23 11:00:02--  https://heibox.uni-heidelberg.de/seafhttp/files/f7e0f408-0e5a-4893-95d6-892ff91c11a4/covid1_lem.txt.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 178483068 (170M) [application/zip]\n",
            "Saving to: â€˜index.html?dl=1â€™\n",
            "\n",
            "index.html?dl=1     100%[===================>] 170.21M  14.9MB/s    in 12s     \n",
            "\n",
            "2021-09-23 11:00:14 (14.5 MB/s) - â€˜index.html?dl=1â€™ saved [178483068/178483068]\n",
            "\n",
            "--2021-09-23 11:00:14--  https://heibox.uni-heidelberg.de/f/76525e4278114c438f1c/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/d49c24a3-89d0-433b-a52d-901c9bc0c6fe/covid2_lem.txt.zip [following]\n",
            "--2021-09-23 11:00:15--  https://heibox.uni-heidelberg.de/seafhttp/files/d49c24a3-89d0-433b-a52d-901c9bc0c6fe/covid2_lem.txt.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177450444 (169M) [application/zip]\n",
            "Saving to: â€˜index.html?dl=1â€™\n",
            "\n",
            "index.html?dl=1     100%[===================>] 169.23M  14.9MB/s    in 12s     \n",
            "\n",
            "2021-09-23 11:00:27 (14.6 MB/s) - â€˜index.html?dl=1â€™ saved [177450444/177450444]\n",
            "\n",
            "--2021-09-23 11:00:27--  https://heibox.uni-heidelberg.de/f/062f72f9e79e4ba5ae4e/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/a39d3159-f50f-44c2-990a-650d1b458cf0/covid3_lem.txt.zip [following]\n",
            "--2021-09-23 11:00:28--  https://heibox.uni-heidelberg.de/seafhttp/files/a39d3159-f50f-44c2-990a-650d1b458cf0/covid3_lem.txt.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177304922 (169M) [application/zip]\n",
            "Saving to: â€˜index.html?dl=1â€™\n",
            "\n",
            "index.html?dl=1     100%[===================>] 169.09M  14.9MB/s    in 11s     \n",
            "\n",
            "2021-09-23 11:00:40 (14.7 MB/s) - â€˜index.html?dl=1â€™ saved [177304922/177304922]\n",
            "\n",
            "--2021-09-23 11:00:40--  https://heibox.uni-heidelberg.de/f/4e290ad989a046caaef4/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/f69b127c-ae83-4923-bdbf-5e6b2fb99a65/covid4_lem.txt.zip [following]\n",
            "--2021-09-23 11:00:41--  https://heibox.uni-heidelberg.de/seafhttp/files/f69b127c-ae83-4923-bdbf-5e6b2fb99a65/covid4_lem.txt.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 177291555 (169M) [application/zip]\n",
            "Saving to: â€˜index.html?dl=1â€™\n",
            "\n",
            "index.html?dl=1     100%[===================>] 169.08M  13.1MB/s    in 12s     \n",
            "\n",
            "2021-09-23 11:00:53 (14.1 MB/s) - â€˜index.html?dl=1â€™ saved [177291555/177291555]\n",
            "\n",
            "--2021-09-23 11:00:53--  https://heibox.uni-heidelberg.de/f/5ec055c8956348d98a44/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/46f8480c-6984-42f9-ae67-da058c9f543d/covid5_lem.txt.zip [following]\n",
            "--2021-09-23 11:00:54--  https://heibox.uni-heidelberg.de/seafhttp/files/46f8480c-6984-42f9-ae67-da058c9f543d/covid5_lem.txt.zip\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 165382404 (158M) [application/zip]\n",
            "Saving to: â€˜index.html?dl=1â€™\n",
            "\n",
            "index.html?dl=1     100%[===================>] 157.72M  14.9MB/s    in 11s     \n",
            "\n",
            "2021-09-23 11:01:05 (14.6 MB/s) - â€˜index.html?dl=1â€™ saved [165382404/165382404]\n",
            "\n",
            "Archive:  file1.zip\n",
            "  inflating: covid1_lem.txt          \n",
            "Archive:  file2.zip\n",
            "  inflating: covid2_lem.txt          \n",
            "Archive:  file3.zip\n",
            "  inflating: covid3_lem.txt          \n",
            "Archive:  file4.zip\n",
            "  inflating: covid4_lem.txt          \n",
            "Archive:  file5.zip\n",
            "  inflating: covid5_lem.txt          \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5f79XcHIFKf"
      },
      "source": [
        "!rm file*.zip\n",
        "# choose if you want to run on a full corpus or part of it\n",
        "# !cat covid1_lem.txt covid2_lem.txt covid3_lem.txt covid4_lem.txt covid5_lem.txt >covid_lem.txt\n",
        "!cat covid1_lem.txt >covid_lem.txt\n",
        "!cp covid_lem.txt /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt\n",
        "!head --lines=15 /usr/local/lib/python3.7/dist-packages/gensim/test/test_data/myOwnLangText8.txt\n",
        "\n",
        "!head --lines=10 covid_lem.txt\n",
        "!wc covid_lem.txt\n",
        "!rm covid_lem.txt\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AJ9guGSNkyw"
      },
      "source": [
        "### Stage6: Training own model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCj_-BYcOe7A"
      },
      "source": [
        "optional: to clean disk space, we remove the downloaded file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnO8k3CWOdqK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dda147a8-4ac1-403f-ad5a-7a157795fcee"
      },
      "source": [
        "# optional: to clean disk space, we remove the downloaded file\n",
        "!rm index.html\\?dl\\=1"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'index.html?dl=1': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AAzBAWaPQZB"
      },
      "source": [
        "from gensim.test.utils import datapath\n",
        "from gensim import utils\n",
        "\n",
        "class MyCorpus:\n",
        "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
        "\n",
        "    def __iter__(self):\n",
        "        corpus_path = datapath('myOwnLangText8.txt')\n",
        "        for line in open(corpus_path):\n",
        "            # assume there's one document per line, tokens separated by whitespace\n",
        "            yield utils.simple_preprocess(line)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9VFjEuyQQ1u"
      },
      "source": [
        "import gensim.models\n",
        "corpusOwn = MyCorpus()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DARUsusgQyWO"
      },
      "source": [
        "optional: examining what is in the corpus after standard preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tLeaX0rQiD7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17f75d18-fee9-4b0c-f05c-6e888bfc9581"
      },
      "source": [
        "# Optional: Examining our corpus format\n",
        "type(corpusOwn)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "__main__.MyCorpus"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSzwUYw_QxYD"
      },
      "source": [
        "dataOwn = [d for d in corpusOwn]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YWSko1SRLJl"
      },
      "source": [
        "print(len(dataOwn))\n",
        "print(dataOwn[0])\n",
        "print(len(dataOwn[0]))\n",
        "print(dataOwn[4])\n",
        "print(len(dataOwn[4]))\n",
        "print(dataOwn[5])\n",
        "print(len(dataOwn[5]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "468wJ6YkVNad"
      },
      "source": [
        "... Initialising global parameters for our modelL vector size, collocation window, skip-grams, negative sampling...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJBwk5p9K4C0"
      },
      "source": [
        "del dataOwn"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8Eep9wyHh9d"
      },
      "source": [
        "from gensim.models import Word2Vec # The word2vec model class\n",
        "import gensim.downloader as api # Allows us to download some free training data"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xObhQQ2SVBDn"
      },
      "source": [
        "vector_size = 100 # Dimensionality of the word vectors\n",
        "window = 5 # How many words either side? (5 = 5 context words either side, i.e. 10 context words in total)\n",
        "use_skip_gram = 1 # If you set this to 0, then it will create a 'continuous bag of words' model instead\n",
        "use_softmax = 0 # If you set this to 1, then hierarchical softmax will be used instead of negative sampling\n",
        "negative_samples = 5 # How many incorrect answers to generate per correct answer when negative sampling\n",
        "\n",
        "modelOwn = Word2Vec(\n",
        "    size=vector_size,\n",
        "    window=window,\n",
        "    sg=use_skip_gram,\n",
        "    hs=use_softmax,\n",
        "    negative=negative_samples\n",
        ")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_QILgoIVeea"
      },
      "source": [
        "... this cell may run for ~2 min or so..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaecmdkPVLCh"
      },
      "source": [
        "modelOwn.build_vocab(corpusOwn)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEdwCKEVVnGv"
      },
      "source": [
        "THIS MAY TAKE LONG!!! ... training the model may take 9 to 15 minutes... (just grab a cup of coffee or a sandwich while you are waiting... You may try chaning the number of epochs; if the number is lower the training is faster, but the quality may be lower...\n",
        "\n",
        "5 epochs = 50 min\n",
        "8 epochs = 1 h 17 min (77 min)\n",
        "10 epochs ~? 100 min ~ 1h 40 min?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrvDUrOJVxOG"
      },
      "source": [
        "modelOwn.train(sentences=corpusOwn, epochs=10, total_examples=modelOwn.corpus_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trrJqAeUWWmL"
      },
      "source": [
        "Now we copy word vectors and remove the model from memory (just to free up the resources...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2GbyMw5WVZd"
      },
      "source": [
        "word_vectors_own = modelOwn.wv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjLyitW5Whff"
      },
      "source": [
        "del modelOwn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_AGZILaYvAJ",
        "outputId": "ab9c6772-dc1e-446f-ced4-289a6d9e0e67"
      },
      "source": [
        "vector = word_vectors_own['however']\n",
        "print(vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.05926263 -0.15667033  0.03318803 -0.18686087 -0.01929474  0.0850039\n",
            " -0.45187265 -0.06674725 -0.21036066  0.15188602 -0.11955456 -0.24665129\n",
            " -0.17086191  0.1575264   0.1435385   0.05060921  0.13252607  0.05781075\n",
            " -0.2439971  -0.21926726  0.17216878  0.08300912  0.01004632  0.07455021\n",
            "  0.1758465   0.09111493  0.27284402 -0.05395681 -0.00918299 -0.18407369\n",
            " -0.17696461  0.2809646  -0.14535953 -0.09520941  0.05423077  0.32214114\n",
            "  0.15689044 -0.01155012  0.22412404  0.17828822  0.23961754 -0.1286369\n",
            " -0.03883589  0.1639629  -0.14529589 -0.40622824  0.12130663  0.21533312\n",
            " -0.05149914  0.07464544  0.02623041  0.20257315  0.02297816 -0.04642959\n",
            " -0.30257323  0.10443529  0.10155771 -0.04316276  0.06895683 -0.15213935\n",
            "  0.02633966  0.19673464 -0.10915372  0.24391913 -0.34283364 -0.33969817\n",
            " -0.09345365  0.06161407  0.2184848   0.18220435  0.00633711 -0.20755926\n",
            " -0.09029377  0.31810436  0.17724325 -0.04032632  0.13063999  0.10723896\n",
            "  0.06444924  0.22883163  0.11574932  0.0286354   0.33301505  0.00929756\n",
            "  0.23733549 -0.3116347  -0.06416965  0.30950138 -0.17339744 -0.12694962\n",
            "  0.15205944  0.1024484   0.34145615  0.03361854 -0.11264163  0.35280678\n",
            " -0.2954409  -0.261902    0.3749097   0.15058918]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgbKj9mUY2Ic",
        "outputId": "4acc39ef-17a4-490a-8185-6c5fbf5f0069"
      },
      "source": [
        "similar_words = word_vectors_own.most_similar('however', topn=30)\n",
        "print('\\n'.join([str(tup) for tup in similar_words]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('although', 0.9558818340301514)\n",
            "('nevertheless', 0.944508969783783)\n",
            "('nonetheless', 0.9050902128219604)\n",
            "('moreover', 0.861580491065979)\n",
            "('furthermore', 0.8573136925697327)\n",
            "('unfortunately', 0.8478303551673889)\n",
            "('though', 0.8315995335578918)\n",
            "('indeed', 0.8223739266395569)\n",
            "('while', 0.8093974590301514)\n",
            "('still', 0.805526614189148)\n",
            "('surprisingly', 0.8004773855209351)\n",
            "('but', 0.7992550134658813)\n",
            "('additionally', 0.7867470979690552)\n",
            "('despite', 0.7771902084350586)\n",
            "('therefore', 0.7697354555130005)\n",
            "('likewise', 0.7666776776313782)\n",
            "('besides', 0.7603017091751099)\n",
            "('importantly', 0.7583843469619751)\n",
            "('notably', 0.7536568641662598)\n",
            "('thirdly', 0.7478072047233582)\n",
            "('notwithstanding', 0.7448851466178894)\n",
            "('fortunately', 0.7411450147628784)\n",
            "('lastly', 0.7401111721992493)\n",
            "('meanwhile', 0.7368455529212952)\n",
            "('interestingly', 0.736506998538971)\n",
            "('because', 0.7339373230934143)\n",
            "('thus', 0.7322976589202881)\n",
            "('since', 0.7287991046905518)\n",
            "('whilst', 0.7268706560134888)\n",
            "('yet', 0.7228360772132874)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-ipjoSmWtOR"
      },
      "source": [
        "Now we can examine the output of our word embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q60fDwHXI3RL"
      },
      "source": [
        "# x is to king as woman is to man\n",
        "# x = small + biggest - big\n",
        "# ğ‘¥âˆ’ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²ğ‘ ğ‘šğ‘ğ‘™ğ‘™â€²)=ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²ğ‘ğ‘–ğ‘”ğ‘”ğ‘’ğ‘ ğ‘¡â€²)âˆ’ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²ğ‘ğ‘–ğ‘”â€²) \n",
        "# âˆ´ ğ‘¥=ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²ğ‘ ğ‘šğ‘ğ‘™ğ‘™â€²)+ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²ğ‘ğ‘–ğ‘”ğ‘”ğ‘’ğ‘ ğ‘¡â€²)âˆ’ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²ğ‘ğ‘–ğ‘”â€²) \n",
        "# âˆ´ ğ‘¥=ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²ÖƒÕ¸Ö„Ö€â€²)+ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²Õ¡Õ´Õ¥Õ¶Õ¡Õ´Õ¥Õ®Õ¨â€²)âˆ’ğ‘£ğ‘’ğ‘ğ‘¡ğ‘œğ‘Ÿ(â€²Õ´Õ¥Õ®â€²) \n",
        "analogous_words = word_vectors_own.most_similar(negative=['nurse'], positive=['doctor','attendant'])\n",
        "print('\\n'.join([str(tup) for tup in analogous_words]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czmhvtc-JB4C"
      },
      "source": [
        "# try your own examples... (also -- morphology?)\n",
        "# # x is to daughter as people is to person (plural + daughter)\n",
        "# analogous_words = word_vectors.most_similar(negative=['person'], positive=['people','daughter'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}